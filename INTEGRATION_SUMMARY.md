# ‚úÖ Django Backend Integration Complete

## Summary of Changes

### 1. **Backend Changes** (`backend/pc_diagnostic/views.py`)

‚úÖ **Added `predict()` function** - Main endpoint for AI reasoning model
- Handles requests to the LLM API at `https://3ccc9499bbff.ngrok-free.app`
- Accepts `input_text` and optional `telemetry_data`
- Returns comprehensive diagnostic responses
- Processing time: 30-120 seconds

‚úÖ **Added `upload_file()` function** - File upload handler
- Accepts multipart form data
- Saves files to `media/uploads/`
- Ready for future file-based diagnostics

‚úÖ **Updated dependencies** (`requirements.txt`)
- Added `requests==2.31.0` for HTTP calls

### 2. **API Endpoints**

| Endpoint | Method | Purpose | Response Time |
|----------|--------|---------|---------------|
| `/api/diagnose/` | POST | Simple diagnostic (mock) | Instant |
| `/api/predict/` | POST | AI reasoning model | 30-120 sec |
| `/api/upload/` | POST | File upload handler | Instant |

### 3. **Frontend Integration Files Created**

üìÑ **DiagnosticChat.js** - Complete chat component with:
- Real-time message display
- Markdown formatting
- Loading states
- Error handling
- Token usage display

üìÑ **App_with_predict.js** - Enhanced App.js example showing:
- How to call the `/api/predict/` endpoint
- Message formatting for markdown
- Toggle between simple/reasoning modes
- Proper loading indicators

üìÑ **API_DATA_FLOW.md** - Complete documentation of:
- Request/response formats
- Data structures
- Field explanations
- Frontend usage examples
- Error handling

---

## üìä What Data is Sent to Frontend

### Primary Response Structure
```json
{
  "success": true,
  "message": "**Analysis**\n\nBased on your issue...",  // ‚Üê DISPLAY THIS
  "model": "reasoning-llama-3.1-cot-re1-nmt-v2-orpo-i1",
  "finish_reason": "stop",
  "usage": {
    "prompt_tokens": 1524,
    "completion_tokens": 651,
    "total_tokens": 2175
  }
}
```

### Key Field: `message`
This is the **main content** to display to users. It contains:
- Problem analysis
- Root cause identification
- Step-by-step solutions
- Recommendations
- Technical details

### Format: Markdown-style Text
```
**Analysis**
Based on the provided telemetry data...

**Root Cause**
Upon closer inspection...

**Solutions**

### 1. Update Graphics Card Drivers
Update the NVIDIA GeForce RTX 2050 drivers...

### 2. Disable Power-Saving Features
*   **Power saver mode**: Ensure this feature is turned off.
*   **Screen brightness adjustment**: Check if automatic...
```

---

## üöÄ How to Use in React

### Simple Example
```javascript
const response = await fetch('http://localhost:8000/api/predict/', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    input_text: "my screen is flickering"
  })
});

const data = await response.json();

if (data.success) {
  // Display the AI's message
  console.log(data.message);  // ‚Üê This is what you show to users
}
```

### With State Management
```javascript
const [aiResponse, setAiResponse] = useState('');
const [isLoading, setIsLoading] = useState(false);

const getAIDiagnosis = async (userInput) => {
  setIsLoading(true);
  
  const response = await fetch('http://localhost:8000/api/predict/', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ input_text: userInput })
  });

  const data = await response.json();
  
  if (data.success) {
    setAiResponse(data.message);  // ‚Üê The actual AI response
  }
  
  setIsLoading(false);
};
```

---

## ‚öôÔ∏è Configuration

### Backend (Django)
```python
# In views.py
LLM_API_BASE = "https://3ccc9499bbff.ngrok-free.app"
LLM_MODEL_ID = "reasoning-llama-3.1-cot-re1-nmt-v2-orpo-i1"
```

### Frontend (React)
```javascript
// Change this to toggle between simple/reasoning mode
const USE_REASONING_MODEL = true;

// API endpoint
const API_URL = 'http://localhost:8000/api/predict/';
```

---

## üé® Display Recommendations

### 1. **Show Loading Indicator**
The model takes 30-120 seconds, so show:
```
ü§ñ Analyzing your issue...
This may take up to 2 minutes.
```

### 2. **Format the Message**
Convert markdown to HTML:
- `**Text**` ‚Üí `<strong>Text</strong>`
- `### Header` ‚Üí `<h3>Header</h3>`
- `*   Item` ‚Üí `<li>Item</li>`

### 3. **Show Metadata (Optional)**
```
Model: reasoning-llama-3.1-cot-re1-nmt-v2-orpo-i1
Tokens: 2,175 (1,524 in + 651 out)
```

---

## üìù Testing the Endpoint

### Using curl
```bash
curl -X POST http://localhost:8000/api/predict/ \
  -H "Content-Type: application/json" \
  -d '{"input_text": "my screen is flickering"}'
```

### Expected Response
```json
{
  "success": true,
  "message": "**Analysis**\n\nBased on...",
  ...
}
```

---

## üîß Troubleshooting

### Issue: "Could not connect to local model server"
**Solution**: Ensure the ngrok tunnel is running at the configured URL

### Issue: "Request timed out"
**Solution**: The model is still processing. Consider increasing timeout or checking server logs

### Issue: "CORS error"
**Solution**: Verify `CORS_ALLOWED_ORIGINS` in `settings.py` includes your frontend URL

---

## üìö Files Reference

| File | Purpose |
|------|---------|
| `backend/pc_diagnostic/views.py` | Django views with predict endpoint |
| `backend/pc_diagnostic/urls.py` | API route definitions |
| `backend/requirements.txt` | Python dependencies |
| `frontend/src/App_with_predict.js` | Example React integration |
| `frontend/src/components/DiagnosticChat.js` | Full-featured chat component |
| `API_DATA_FLOW.md` | Complete API documentation |

---

## ‚ú® Next Steps

1. **Test the endpoint**: Run Django server and test `/api/predict/`
2. **Update frontend**: Use `App_with_predict.js` as reference
3. **Add telemetry**: Include system data in requests for better diagnostics
4. **Enhance UI**: Use the DiagnosticChat component for a polished interface

---

**Need help?** Check `API_DATA_FLOW.md` for detailed documentation!
