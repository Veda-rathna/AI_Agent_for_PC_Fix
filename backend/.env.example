# ========================================
# AI-Driven PC Diagnostic Assistant
# Environment Configuration
# ========================================

# LLM Provider Configuration
# Options: "gemini" (Google Gemini) or "local" (Local LLaMA)
LLM_PROVIDER=gemini

# ========================================
# Google Gemini Configuration
# ========================================
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_google_gemini_api_key_here

# Gemini Model Selection
# Options: gemini-2.5-flash, gemini-2.5-pro, gemini-flash-latest
# Recommended: gemini-2.5-flash (fast and cost-effective)
GEMINI_MODEL=gemini-2.5-flash

# ========================================
# Local LLaMA Configuration (Fallback)
# ========================================
# Local llama.cpp server API endpoint
LLAMA_API_BASE=http://127.0.0.1:1234

# Model ID for llama.cpp server
LLAMA_MODEL_ID=reasoning-llama-3.1-cot-re1-nmt-v2-orpo-i1

# ========================================
# Instructions
# ========================================
# 1. Copy this file to .env in the backend directory
# 2. Replace the placeholder values with your actual API keys
# 3. Choose your LLM_PROVIDER (gemini or local)
# 4. Never commit the .env file to version control
#
# For Google Gemini integration:
#   - Visit https://aistudio.google.com/app/apikey
#   - Create a new API key
#   - Paste it into GEMINI_API_KEY above
#   - Set LLM_PROVIDER=gemini
#
# For Local LLaMA (offline mode):
#   - Ensure llama.cpp server is running on port 1234
#   - Set LLM_PROVIDER=local
#   - Configure LLAMA_API_BASE if using different port
#
# Fallback chain:
#   Gemini → Local LLaMA → Offline Mock Analysis
