# AutoGen MCP Task Automation Integration

## Overview

This integration adds Microsoft AutoGen to the AI PC Diagnostic Agent to automatically execute MCP (Model Context Protocol) tasks generated by the AI diagnostic model.

## What is MCP?

The AI model generates two types of output:
1. **User-Friendly Message**: Instructions for the user to perform manually
2. **MCP Tasks**: System-level diagnostic commands that can be executed automatically

Example MCP output from the AI:
```
<MCP_TASKS>
{
  "tasks": [
    "Analyze CPU thermal sensor readings for overheating patterns",
    "Inspect disk usage telemetry for signs of resource-intensive programs",
    "Verify Windows Event Logs for error messages related to system crashes",
    "Scan system files for potential corruption using SFC",
    "Check power management settings that could limit performance"
  ],
  "summary": "Perform comprehensive system diagnostics"
}
</MCP_TASKS>
```

## Architecture

```
AI Model (generates diagnosis + MCP tasks)
        ↓
MCP Task Parser (extracts and validates tasks)
        ↓
AutoGen Orchestrator (coordinates execution)
        ↓
    ┌───┴───┐
    ↓       ↓
Direct      AutoGen Agents
Execution   (coordinator + specialists)
    ↓           ↓
Diagnostic Tools (CPU, disk, memory, events, SFC)
        ↓
Results Aggregation
        ↓
Return to User
```

## Directory Structure

```
backend/autogen_integration/
├── __init__.py                 # Module initialization
├── orchestrator.py             # Main orchestration logic
├── agents/
│   ├── __init__.py
│   └── diagnostic_agents.py    # AutoGen agent definitions
├── tools/
│   ├── __init__.py
│   ├── system_diagnostics.py   # CPU, disk, memory, power tools
│   ├── event_logs.py           # Windows Event Log analysis
│   └── file_checker.py         # SFC and DISM tools
├── parsers/
│   ├── __init__.py
│   └── mcp_parser.py           # MCP task extraction and validation
├── config/
│   ├── agents_config.json      # Agent configuration
│   └── llm_config.json         # LLM settings
└── logs/
    └── autogen_execution.log   # Execution logs
```

## Installation

1. **Install dependencies:**
```bash
cd backend
pip install -r requirements.txt
```

The requirements include:
- `pyautogen==0.2.18` - AutoGen framework
- `openai==1.12.0` - OpenAI API client
- `autogen-agentchat==0.2.0` - AutoGen chat capabilities

2. **Configure LLM (Optional):**

Edit `backend/autogen_integration/config/llm_config.json`:
```json
{
  "llm_config": {
    "config_list": [{
      "model": "gpt-3.5-turbo",
      "api_key": "YOUR_API_KEY_HERE"
    }],
    "temperature": 0.7
  }
}
```

**Note:** LLM configuration is only needed if using AutoGen agents mode. Direct execution mode (default) doesn't require LLM.

## Execution Modes

### 1. Direct Execution (Default, Recommended)
- ✅ Fast and deterministic
- ✅ No LLM API calls required
- ✅ No additional costs
- Direct mapping of tasks to diagnostic tools

### 2. AutoGen Agents Mode
- Uses intelligent agents to execute tasks
- Requires LLM configuration
- More flexible but slower
- Additional API costs

## API Endpoints

### 1. Execute MCP Tasks
```http
POST /api/mcp/execute/
Content-Type: application/json

{
  "model_output": "Full AI response including <MCP_TASKS>",
  "execution_mode": "direct"  // or "autogen"
}
```

**Response:**
```json
{
  "success": true,
  "tasks_requested": 5,
  "tasks_completed": 5,
  "tasks_failed": 0,
  "summary": "Perform comprehensive diagnostics",
  "results": [
    {
      "success": true,
      "task": "CPU Thermal Analysis",
      "data": {...},
      "analysis": "✅ CPU temperature normal",
      "severity": "low"
    }
  ],
  "execution_summary": "Formatted summary...",
  "execution_timestamp": "2025-10-31T..."
}
```

### 2. Parse MCP Tasks (Without Execution)
```http
POST /api/mcp/parse/
Content-Type: application/json

{
  "model_output": "Full AI response"
}
```

**Response:**
```json
{
  "success": true,
  "tasks": ["task1", "task2", ...],
  "summary": "Task summary",
  "task_count": 5,
  "categorized_tasks": {
    "thermal": ["CPU task"],
    "disk": ["Disk task"],
    "event_log": ["Event log task"]
  }
}
```

### 3. Get Orchestrator Status
```http
GET /api/mcp/status/
```

**Response:**
```json
{
  "success": true,
  "orchestrator_available": true,
  "execution_modes": ["direct", "autogen"],
  "recommended_mode": "direct",
  "available_tools": [
    "analyze_cpu_thermal",
    "inspect_disk_usage",
    "check_power_settings",
    "check_memory_usage",
    "verify_event_logs",
    "scan_system_files",
    "check_dism_health"
  ]
}
```

### 4. Integrated Execution (Predict Endpoint)
```http
POST /api/predict/
Content-Type: application/json

{
  "input_text": "My computer is running slow",
  "execute_mcp_tasks": true  // Auto-execute MCP tasks
}
```

**Response includes MCP execution results:**
```json
{
  "success": true,
  "message": "AI diagnostic response...",
  "mcp_execution": {
    "executed": true,
    "tasks_completed": 5,
    "results": [...],
    "execution_summary": "..."
  }
}
```

## Available Diagnostic Tools

### System Diagnostics
1. **analyze_cpu_thermal()** - CPU temperature and usage analysis
2. **inspect_disk_usage()** - Disk space and I/O analysis
3. **check_power_settings()** - Power management configuration
4. **check_memory_usage()** - RAM usage and process analysis

### Security & Integrity
5. **verify_event_logs()** - Windows Event Log analysis
6. **scan_system_files()** - SFC system file integrity check
7. **check_dism_health()** - Windows image health check

## Task Categorization

Tasks are automatically categorized by keywords:

| Category | Keywords | Example Task |
|----------|----------|--------------|
| thermal | cpu, thermal, temperature | "Analyze CPU thermal sensors" |
| disk | disk, storage, drive | "Inspect disk usage patterns" |
| memory | memory, ram | "Check memory usage" |
| power | power, battery | "Verify power management settings" |
| event_log | event, log, crash, bsod | "Check Windows Event Logs" |
| system_files | sfc, corruption, integrity | "Scan system files with SFC" |
| network | network, wifi, internet | "Check network connectivity" |
| gpu | gpu, graphics, display | "Verify GPU driver status" |

## Usage Examples

### Example 1: Direct Execution (Recommended)
```python
from autogen_integration.orchestrator import AutoGenOrchestrator

orchestrator = AutoGenOrchestrator()

# Model output with MCP tasks
model_output = """
Your computer seems slow. Try closing unnecessary programs.

<MCP_TASKS>
{
  "tasks": [
    "Analyze CPU thermal sensor readings",
    "Check memory usage for resource-intensive processes"
  ],
  "summary": "Check for performance bottlenecks"
}
</MCP_TASKS>
"""

# Execute tasks directly
result = orchestrator.execute_mcp_tasks(model_output, use_autogen=False)

print(f"Completed: {result['tasks_completed']}")
for task_result in result['results']:
    print(f"- {task_result['task']}: {task_result.get('analysis', 'Done')}")
```

### Example 2: Using API
```bash
curl -X POST http://localhost:8000/api/mcp/execute/ \
  -H "Content-Type: application/json" \
  -d '{
    "model_output": "...<MCP_TASKS>...</MCP_TASKS>",
    "execution_mode": "direct"
  }'
```

### Example 3: Frontend Integration (React)
```javascript
const executeMCPTasks = async (modelOutput) => {
  const response = await fetch('/api/mcp/execute/', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model_output: modelOutput,
      execution_mode: 'direct'
    })
  });
  
  const data = await response.json();
  
  if (data.success) {
    console.log(`Executed ${data.tasks_completed} tasks`);
    data.results.forEach(result => {
      console.log(`${result.task}: ${result.analysis}`);
    });
  }
};
```

## Configuration

### Execution Mode Configuration
Edit `config/agents_config.json`:
```json
{
  "execution_mode": "semi-automated",
  "agents": {
    "user_proxy": {
      "human_input_mode": "NEVER"
    },
    "security_agent": {
      "require_admin_confirmation": true
    }
  }
}
```

### Logging Configuration
```json
{
  "logging": {
    "level": "INFO",
    "file": "logs/autogen_execution.log",
    "console": true
  }
}
```

## Security Considerations

1. **Admin Privileges**: Some tools (SFC, DISM) require administrator rights
2. **Automatic Execution**: Review tasks before enabling auto-execution in production
3. **API Keys**: If using AutoGen agents, secure your LLM API keys
4. **Rate Limiting**: Consider implementing rate limits for API endpoints

## Troubleshooting

### "AutoGen orchestrator not available"
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Check logs in `backend/autogen_integration/logs/`

### "No MCP_TASKS found"
- Verify the AI model is generating proper `<MCP_TASKS>` format
- Use `/api/mcp/parse/` to test task extraction

### Admin-required tasks failing
- Run the backend as Administrator for SFC/DISM scans
- Or handle these tasks manually per user instructions

### Import errors
- Ensure Python path includes `backend/` directory
- Check that `__init__.py` files exist in all modules

## Performance

**Direct Execution Mode:**
- Average execution time: 2-5 seconds for 5 tasks
- No external API calls
- Deterministic results

**AutoGen Agents Mode:**
- Average execution time: 10-30 seconds
- Requires LLM API calls
- More flexible but slower

## Future Enhancements

- [ ] Parallel task execution for independent tasks
- [ ] Task dependency resolution
- [ ] Persistent task history and analytics
- [ ] Web UI for manual task approval
- [ ] Custom tool registration
- [ ] Multi-language support

## License

Same as main project

## Support

For issues or questions:
1. Check logs in `backend/autogen_integration/logs/`
2. Review this documentation
3. Create an issue in the repository
